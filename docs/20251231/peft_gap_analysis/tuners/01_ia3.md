# IA3 (Infused Adapter by Inhibiting and Amplifying Activations)

## Overview

IA3 learns multiplicative scaling vectors instead of low-rank matrices. It's simpler than LoRA and has minimal parameter overhead (one vector per layer).

## Python Reference

**Files:**
- `peft/src/peft/tuners/ia3/config.py` (~120 lines)
- `peft/src/peft/tuners/ia3/layer.py` (~280 lines)
- `peft/src/peft/tuners/ia3/model.py` (~200 lines)
- `peft/src/peft/tuners/ia3/bnb.py` (~150 lines) - Quantization support

### IA3Config

```python
@dataclass
class IA3Config(PeftConfig):
    target_modules: Optional[Union[list[str], str]] = None
    feedforward_modules: Optional[Union[list[str], str]] = None
    exclude_modules: Optional[Union[list[str], str]] = None
    fan_in_fan_out: bool = False
    modules_to_save: Optional[list[str]] = None
    init_ia3_weights: bool = True
```

**Key Concepts:**
- `target_modules`: Modules to apply IA3 to (attention layers)
- `feedforward_modules`: Subset where vectors multiply inputs (not outputs)
- `init_ia3_weights`: True = initialize to ones (identity), False = random

### IA3Layer

```python
class IA3Layer(BaseTunerLayer):
    # Learnable vectors (one per adapter)
    ia3_l: dict[str, nn.Parameter]  # Shape: (out_features,) or (in_features,)

    def __init__(self, base_layer, is_feedforward):
        self.is_feedforward = is_feedforward  # Determines vector dimension

    def update_layer(self, adapter_name, init_ia3_weights):
        # Create ia3_l vector
        if self.is_feedforward:
            weight = nn.Parameter(torch.ones(self.in_features))
        else:
            weight = nn.Parameter(torch.ones(self.out_features))
        self.ia3_l[adapter_name] = weight

    def forward(self, x, *args, **kwargs):
        # Base forward
        result = self.base_layer(x, *args, **kwargs)

        # Apply scaling
        if self.is_feedforward:
            # Scale input: result = base_layer(x * ia3_l)
            # Actually computed as: ia3_l * result (mathematically equivalent for linear)
            return result * self.ia3_l[self.active_adapter]
        else:
            # Scale output: result = result * ia3_l
            return result * self.ia3_l[self.active_adapter]
```

### Merge/Unmerge

```python
def merge(self, safe_merge=False, adapter_names=None):
    # For Linear: W' = W * diag(ia3_l)
    # Modifies base layer weight directly
    base_layer = self.get_base_layer()
    for adapter in adapter_names:
        ia3_l = self.ia3_l[adapter]
        if self.is_feedforward:
            # Multiply columns
            base_layer.weight.data *= ia3_l.view(1, -1)
        else:
            # Multiply rows
            base_layer.weight.data *= ia3_l.view(-1, 1)

def unmerge(self):
    # Divide by ia3_l to reverse
    for adapter in self.merged_adapters:
        ia3_l = self.ia3_l[adapter]
        if self.is_feedforward:
            base_layer.weight.data /= ia3_l.view(1, -1)
        else:
            base_layer.weight.data /= ia3_l.view(-1, 1)
```

## Elixir Implementation Design

### Module: `HfPeftEx.Tuners.IA3.Config`

```elixir
defmodule HfPeftEx.Tuners.IA3.Config do
  @moduledoc """
  Configuration for IA3 (Infused Adapter by Inhibiting and Amplifying Activations).
  """

  @derive Jason.Encoder
  defstruct [
    # Base config fields
    peft_type: :ia3,
    task_type: nil,
    base_model_name_or_path: nil,
    inference_mode: false,
    # IA3-specific
    target_modules: nil,
    feedforward_modules: nil,
    exclude_modules: nil,
    fan_in_fan_out: false,
    modules_to_save: nil,
    init_ia3_weights: true
  ]

  @type t :: %__MODULE__{
    peft_type: :ia3,
    task_type: atom() | nil,
    base_model_name_or_path: String.t() | nil,
    inference_mode: boolean(),
    target_modules: [String.t()] | String.t() | nil,
    feedforward_modules: [String.t()] | String.t() | nil,
    exclude_modules: [String.t()] | String.t() | nil,
    fan_in_fan_out: boolean(),
    modules_to_save: [String.t()] | nil,
    init_ia3_weights: boolean()
  }

  @spec new(keyword()) :: t()
  def new(opts \\ []) do
    struct(__MODULE__, opts)
    |> validate!()
  end

  defp validate!(config) do
    # feedforward_modules must be subset of target_modules
    if config.feedforward_modules && config.target_modules do
      ff_set = MapSet.new(List.wrap(config.feedforward_modules))
      target_set = MapSet.new(List.wrap(config.target_modules))
      unless MapSet.subset?(ff_set, target_set) do
        raise ArgumentError, "feedforward_modules must be a subset of target_modules"
      end
    end
    config
  end
end
```

### Module: `HfPeftEx.Tuners.IA3.Layer`

```elixir
defmodule HfPeftEx.Tuners.IA3.Layer do
  @moduledoc """
  IA3 layer implementation with multiplicative scaling vectors.
  """

  import Nx.Defn

  defstruct [
    :base_layer,          # Original layer weights/params
    :in_features,
    :out_features,
    :is_feedforward,
    :fan_in_fan_out,
    ia3_l: %{},           # %{adapter_name => Nx.Tensor}
    active_adapter: "default",
    merged_adapters: [],
    disable_adapters: false,
    merged: false
  ]

  @type t :: %__MODULE__{}

  @doc """
  Create a new IA3 layer wrapping a base layer.
  """
  @spec new(map(), boolean(), keyword()) :: t()
  def new(base_layer, is_feedforward, opts \\ []) do
    {in_features, out_features} = get_dimensions(base_layer, Keyword.get(opts, :fan_in_fan_out, false))

    %__MODULE__{
      base_layer: base_layer,
      in_features: in_features,
      out_features: out_features,
      is_feedforward: is_feedforward,
      fan_in_fan_out: Keyword.get(opts, :fan_in_fan_out, false),
      ia3_l: %{},
      active_adapter: "default"
    }
  end

  @doc """
  Add an adapter to this layer.
  """
  @spec update_layer(t(), String.t(), boolean()) :: t()
  def update_layer(layer, adapter_name, init_ia3_weights \\ true) do
    dim = if layer.is_feedforward, do: layer.in_features, else: layer.out_features

    ia3_l = if init_ia3_weights do
      Nx.broadcast(1.0, {dim})
    else
      Nx.random_normal({dim}, 0.0, 0.02)
    end

    put_in(layer.ia3_l[adapter_name], ia3_l)
  end

  @doc """
  Forward pass with IA3 scaling.
  """
  defn forward(layer, x) do
    # Get base layer output
    base_output = apply_base_layer(layer.base_layer, x)

    if layer.disable_adapters or layer.merged do
      base_output
    else
      ia3_l = layer.ia3_l[layer.active_adapter]
      apply_ia3_scaling(base_output, ia3_l, layer.is_feedforward)
    end
  end

  defnp apply_ia3_scaling(output, ia3_l, is_feedforward) do
    # For both feedforward and non-feedforward, we multiply output
    # The difference is in which dimension the vector is applied
    output * ia3_l
  end

  @doc """
  Merge IA3 scaling into base layer weights.
  """
  @spec merge(t(), keyword()) :: {:ok, t()} | {:error, term()}
  def merge(layer, opts \\ []) do
    adapter_names = Keyword.get(opts, :adapter_names, [layer.active_adapter])

    merged_layer = Enum.reduce(adapter_names, layer, fn adapter, acc ->
      ia3_l = acc.ia3_l[adapter]
      base_weight = acc.base_layer.weight

      new_weight = if acc.is_feedforward do
        # Multiply columns: W[:, i] *= ia3_l[i]
        Nx.multiply(base_weight, Nx.reshape(ia3_l, {1, :auto}))
      else
        # Multiply rows: W[i, :] *= ia3_l[i]
        Nx.multiply(base_weight, Nx.reshape(ia3_l, {:auto, 1}))
      end

      acc
      |> put_in([:base_layer, :weight], new_weight)
      |> update_in([:merged_adapters], &[adapter | &1])
    end)

    {:ok, %{merged_layer | merged: true}}
  end

  @doc """
  Unmerge IA3 scaling from base layer weights.
  """
  @spec unmerge(t()) :: {:ok, t()} | {:error, term()}
  def unmerge(layer) do
    unmerged_layer = Enum.reduce(layer.merged_adapters, layer, fn adapter, acc ->
      ia3_l = acc.ia3_l[adapter]
      base_weight = acc.base_layer.weight

      new_weight = if acc.is_feedforward do
        Nx.divide(base_weight, Nx.reshape(ia3_l, {1, :auto}))
      else
        Nx.divide(base_weight, Nx.reshape(ia3_l, {:auto, 1}))
      end

      put_in(acc, [:base_layer, :weight], new_weight)
    end)

    {:ok, %{unmerged_layer | merged: false, merged_adapters: []}}
  end

  defp get_dimensions(base_layer, fan_in_fan_out) do
    {out_features, in_features} = Nx.shape(base_layer.weight)
    if fan_in_fan_out do
      {out_features, in_features}
    else
      {in_features, out_features}
    end
  end

  defp apply_base_layer(base_layer, x) do
    # Simple linear: x @ W^T + b
    Nx.dot(x, Nx.transpose(base_layer.weight)) |> maybe_add_bias(base_layer)
  end

  defp maybe_add_bias(output, %{bias: nil}), do: output
  defp maybe_add_bias(output, %{bias: bias}), do: Nx.add(output, bias)
end
```

## Files to Read

**Python (required reading):**
- `peft/src/peft/tuners/ia3/config.py`
- `peft/src/peft/tuners/ia3/layer.py`
- `peft/src/peft/tuners/ia3/model.py`
- `peft/src/peft/tuners/ia3/__init__.py`

**Elixir (context):**
- `lib/hf_peft_ex/tuners/lora/layer.ex`
- `lib/hf_peft_ex/tuners/lora/linear.ex`
- `lib/hf_peft_ex/config.ex`

## Tests Required

1. **Config Tests:**
   - Create config with defaults
   - Validate feedforward_modules subset constraint
   - JSON serialization/deserialization

2. **Layer Tests:**
   - Initialize layer with ones (identity)
   - Forward pass returns identity when ia3_l = ones
   - Forward pass scales correctly
   - Feedforward vs non-feedforward distinction

3. **Merge/Unmerge Tests:**
   - Merge modifies base weights correctly
   - Unmerge restores original weights
   - Round-trip: merge + unmerge = identity

4. **Multi-adapter Tests:**
   - Add multiple adapters
   - Switch between adapters
   - Delete adapter

## Mathematical Foundation

**IA3 Forward Pass:**
```
For non-feedforward (attention): y = (Wx) * l
For feedforward: y = W(x * l) = (Wx) * l  (for linear layers)
```

**Where:**
- `W` is the base weight matrix
- `x` is input
- `l` is the learned scaling vector
- `*` is element-wise multiplication

**Merge Operation:**
```
W' = W * diag(l)
```

## Parameter Count

For a layer with output dimension `d`:
- IA3 adds only `d` parameters (the scaling vector)
- Compare to LoRA which adds `r * (in + out)` parameters

## Complexity

- **Implementation:** Low (simpler than LoRA)
- **Mathematical:** Low
- **Dependencies:** Base tuner utils

## Priority

**High** - Simple to implement, good test case for tuner infrastructure
